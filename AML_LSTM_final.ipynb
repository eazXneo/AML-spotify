{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb293d4-e146-47b7-9261-cc36faad0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "import time \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad5461-7e06-415e-8d64-b250798b79c7",
   "metadata": {},
   "source": [
    "This file uses LSTM model to predict the following week's features. The training process is using the 2022's data to train and the data of 2023 to validate(Same dataset as Holt-Winters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f7ce72-2e0a-4ef7-8bf5-b45e6c9a0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\Windows\\Desktop\\Spotify_Dataset_V3.csv',delimiter=';')\n",
    "df_noURL=df.iloc[:,0:-1]\n",
    "url=df.iloc[:,-1]\n",
    "# Creat a daily average data of each feature\n",
    "df_nodup = df_noURL.loc[:,['Title','Artists','Date','Danceability','Energy','Loudness','Speechiness','Acousticness','Instrumentalness','Valence']]\n",
    "df_nodup = df_nodup.drop_duplicates(subset=['Title','Artists','Date'])\n",
    "df_nodup = df_nodup.drop('Artists', axis=1)\n",
    "df_nodup = df_nodup.iloc[::-1]\n",
    "Date = [];Mean_dance=[];Mean_energy=[];Mean_loud=[];Mean_speech=[];Mean_acoustic=[];Mean_instru=[];Mean_valence=[]\n",
    "Var_dance=[];Var_energy=[];Var_loud=[];Var_speech=[];Var_acoustic=[];Var_instru=[];Var_valence=[]\n",
    "i=199;k=0\n",
    "while (df_nodup.shape[0]-i)>0:\n",
    "    date=datetime.strptime(df_nodup.iloc[i,1],'%d/%m/%Y').strftime('%Y-%m-%d')\n",
    "    if i>200:\n",
    "        if date == Date[-1]:\n",
    "            k=k+1\n",
    "            i=i+1\n",
    "            continue\n",
    "    Date.append(date)\n",
    "    df_mean=df_nodup.iloc[i-199-k:i,2:].mean()\n",
    "    df_var=df_nodup.iloc[i-199-k:i,2:].var()\n",
    "    Mean_dance.append(df_mean.iloc[0]);Var_dance.append(df_var.iloc[0])\n",
    "    Mean_energy.append(df_mean.iloc[1]);Var_energy.append(df_var.iloc[1])\n",
    "    Mean_loud.append(df_mean.iloc[2]);Var_loud.append(df_var.iloc[2])\n",
    "    Mean_speech.append(df_mean.iloc[3]);Var_speech.append(df_var.iloc[3])\n",
    "    Mean_acoustic.append(df_mean.iloc[4]);Var_acoustic.append(df_var.iloc[4])\n",
    "    Mean_instru.append(df_mean.iloc[5]);Var_instru.append(df_var.iloc[5])\n",
    "    Mean_valence.append(df_mean.iloc[6]);Var_valence.append(df_var.iloc[6])\n",
    "    i+=200\n",
    "    k=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d42733-7f67-489f-bf0a-9a84ed9e346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        L.seed_everything(seed=41)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        for input_i in range(input.size(0)):\n",
    "            input_trans = input[input_i].view(len(input[input_i]),1)\n",
    "            lstm_out, temp = self.lstm(input_trans)\n",
    "            ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "            if input_i==0:\n",
    "                prediction=lstm_out[-1]\n",
    "            else:\n",
    "                prediction=torch.cat((prediction,lstm_out[-1]),0)\n",
    "        #prediction = prediction\n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.001) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "  \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = ((output_i - label_i)**2).mean() ## loss = mean squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        '''\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "        '''\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb0946e-7ac8-42eb-a3da-8fabcf64172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_weekpredict(data, model):\n",
    "    \"\"\"\n",
    "        Returns mse of weeks' prediction\n",
    "    \"\"\"\n",
    "    # errors array\n",
    "    mse_all=[]\n",
    "    for j in range(len(data[0])-37):\n",
    "        predict=np.array([])\n",
    "        if len(data[0])==2:\n",
    "            input_data=[data[0][0][j:j+30],data[0][1][j:j+30]]\n",
    "        else:\n",
    "            input_data=data[0][j:j+30]\n",
    "        for i in range(7):\n",
    "            pred = model(torch.tensor([input_data])).detach()\n",
    "            input_data = np.append(input_data,pred)\n",
    "            predict = np.append(predict,pred)\n",
    "            float32_array = np.array(input_data).astype(np.float32)\n",
    "            # Convert back to a list\n",
    "            input_data = float32_array.tolist()\n",
    "        mse=round(np.power((np.array(data[0][j+30:j+37])-predict),2).mean(),7)\n",
    "        mse_all.append(mse)\n",
    "    return np.array(mse_all).mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058c0002-359e-4455-866a-81b6c8b2429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc146dccacf945bb8f3cdb3558ee9a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.3075],\n",
      "        [-0.2849],\n",
      "        [ 0.8593],\n",
      "        [-0.4408]])\n",
      "lstm.weight_hh_l0 tensor([[-0.0626],\n",
      "        [ 0.9761],\n",
      "        [-0.4540],\n",
      "        [ 0.8971]])\n",
      "lstm.bias_ih_l0 tensor([0.8290, 0.5353, 0.4384, 0.6003])\n",
      "lstm.bias_hh_l0 tensor([ 0.9765,  0.3452, -0.4608,  1.1365])\n",
      "\n",
      "Mse in weeks calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.545803571428571e-05\n",
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff01ec764694790ada0d4a154dc87f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.3153],\n",
      "        [-0.2989],\n",
      "        [ 0.8647],\n",
      "        [-0.4414]])\n",
      "lstm.weight_hh_l0 tensor([[-0.0676],\n",
      "        [ 0.9626],\n",
      "        [-0.4663],\n",
      "        [ 0.8938]])\n",
      "lstm.bias_ih_l0 tensor([0.8210, 0.5228, 0.4446, 0.5936])\n",
      "lstm.bias_hh_l0 tensor([ 0.9685,  0.3327, -0.4546,  1.1297])\n",
      "\n",
      "Mse in weeks calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.017767857142857e-05\n",
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f0cc16e96648a395d76b921f929954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.6654],\n",
      "        [-0.6732],\n",
      "        [ 0.9095],\n",
      "        [-0.7896]])\n",
      "lstm.weight_hh_l0 tensor([[-0.3539],\n",
      "        [ 0.6479],\n",
      "        [-0.8711],\n",
      "        [ 0.5966]])\n",
      "lstm.bias_ih_l0 tensor([0.4686, 0.1434, 0.4842, 0.2562])\n",
      "lstm.bias_hh_l0 tensor([ 0.6162, -0.0466, -0.4150,  0.7923])\n",
      "\n",
      "Mse in weeks calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.432321428571429e-05\n",
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277009576c4249c6ab767bc1f902a709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.4158],\n",
      "        [-0.4826],\n",
      "        [ 0.9383],\n",
      "        [-0.4981]])\n",
      "lstm.weight_hh_l0 tensor([[-0.1754],\n",
      "        [ 0.7067],\n",
      "        [-0.6968],\n",
      "        [ 0.7737]])\n",
      "lstm.bias_ih_l0 tensor([0.7046, 0.3153, 0.5053, 0.4703])\n",
      "lstm.bias_hh_l0 tensor([ 0.8522,  0.1252, -0.3939,  1.0065])\n",
      "\n",
      "Mse in weeks calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013220151785714282\n",
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369189cacba04242948ae05b5cff50e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.7007],\n",
      "        [-0.6990],\n",
      "        [ 0.8799],\n",
      "        [-0.8166]])\n",
      "lstm.weight_hh_l0 tensor([[-0.3480],\n",
      "        [ 0.6531],\n",
      "        [-0.8958],\n",
      "        [ 0.6014]])\n",
      "lstm.bias_ih_l0 tensor([0.4351, 0.1190, 0.4573, 0.2314])\n",
      "lstm.bias_hh_l0 tensor([ 0.5827, -0.0711, -0.4419,  0.7675])\n",
      "\n",
      "Mse in weeks calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 41\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6089285714285714e-06\n",
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.5271],\n",
      "        [-0.5468],\n",
      "        [ 0.6011],\n",
      "        [-0.6616]])\n",
      "lstm.weight_hh_l0 tensor([[-0.4701],\n",
      "        [ 0.5440],\n",
      "        [-0.7436],\n",
      "        [ 0.4904]])\n",
      "lstm.bias_ih_l0 tensor([0.6089, 0.2714, 0.1792, 0.3866])\n",
      "lstm.bias_hh_l0 tensor([ 0.7565,  0.0814, -0.7200,  0.9227])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a126f41a9243089593062b19a19b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[-0.3612],\n",
      "        [-0.3521],\n",
      "        [ 0.8885],\n",
      "        [-0.4924]])\n",
      "lstm.weight_hh_l0 tensor([[-0.0914],\n",
      "        [ 0.9270],\n",
      "        [-0.5238],\n",
      "        [ 0.8631]])\n",
      "lstm.bias_ih_l0 tensor([0.7757, 0.4684, 0.4677, 0.5414])\n",
      "lstm.bias_hh_l0 tensor([ 0.9233,  0.2783, -0.4314,  1.0775])\n",
      "\n",
      "Mse in weeks calculation\n",
      "6.552946428571429e-05\n"
     ]
    }
   ],
   "source": [
    "Daily_mean = {'Mean_dance': Mean_dance,'Mean_energy':Mean_energy,'Mean_speech':Mean_speech,'Mean_acoustic':Mean_acoustic,'Mean_instru':Mean_instru,'Mean_valence':Mean_valence}\n",
    "mse={}\n",
    "for j in list(Daily_mean.keys()):\n",
    "    Train_data=[]\n",
    "    data = Daily_mean[j]\n",
    "    for i in range(31):\n",
    "        train=data[1822+10+i:2296-119-31+10+i]\n",
    "        Train_data.append(train)#2022.1.1-2022.11.20-2022.12.20\n",
    "    Test_data=data[2296-119-31+10:2296-119+10]#2022.11.21-2022.12.21(Prevent chiristmas)\n",
    "    #Train_test_data=Mean_dance[1822:2306-119]#2022.1.1-2022.12.31\n",
    "    Validate_data_input=data[2306-119:]#2023.1.1-2023.5.22                     \n",
    "    #Result_data=Mean_dance[2306-119:]#2023.1.1-2023.5.29 the data from model.result contains 7 days of pure prediction\n",
    "    float32_array = np.array(Train_data).astype(np.float32)\n",
    "    # Convert back to a list\n",
    "    Train_data = float32_array.tolist()\n",
    "    float32_array = np.array(Test_data).astype(np.float32)\n",
    "    # Convert back to a list\n",
    "    Test_data = float32_array.tolist()\n",
    "    float32_array = np.array(Validate_data_input).astype(np.float32)\n",
    "    # Convert back to a list\n",
    "    Validate_data_input = float32_array.tolist()\n",
    "    ## create the training data for the neural network.\n",
    "    inputs = torch.tensor(Train_data)\n",
    "    labels = torch.tensor(Test_data)\n",
    "    dataset = TensorDataset(inputs.unsqueeze(0), labels.unsqueeze(0))\n",
    "    #dataset = TensorDataset(inputs, labels) \n",
    "    dataloader = DataLoader(dataset)\n",
    "    model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "    ## print out the name and value for each parameter\n",
    "    print(\"Before optimization, the parameters are...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.data)\n",
    "    \n",
    "    trainer = L.Trainer(max_epochs=2000)#log_every_n_steps=2\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=dataloader)\n",
    "    \n",
    "    print(\"After optimization, the parameters are...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.data)\n",
    "    print('\\nMse in weeks calculation')\n",
    "    mse[j]=(LSTM_weekpredict([Validate_data_input],model))\n",
    "    print(mse[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d1c40f-c645-464a-adc9-cfa3e228944e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean_dance': 5.545803571428571e-05,\n",
       " 'Mean_energy': 6.017767857142857e-05,\n",
       " 'Mean_speech': 4.432321428571429e-05,\n",
       " 'Mean_acoustic': 0.0013220151785714282,\n",
       " 'Mean_instru': 3.6089285714285714e-06,\n",
       " 'Mean_valence': 6.552946428571429e-05}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mse_all={'Mean_dance': mse[0],'Mean_energy':mse[1],'Mean_speech':mse[2],'Mean_acoustic':mse[3],'Mean_instru':mse[4],'Mean_valence':mse[5]}\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c0be8-3bbe-4813-bb13-184f5ce3af7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
